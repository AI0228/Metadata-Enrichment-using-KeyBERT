{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1fd711e81d6e45b283332c74569b93c9": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d9062e5146d24600b2596f3475658ede",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m  80%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/5 \u001b[0m [ \u001b[33m0:00:19\u001b[0m < \u001b[36m0:00:04\u001b[0m , \u001b[31m0 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  80%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">4/5 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:19</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:04</span> , <span style=\"color: #800000; text-decoration-color: #800000\">0 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "d9062e5146d24600b2596f3475658ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUi5qAsa_Rfa",
        "outputId": "6b0b49fb-2885-45c8-f81a-0db774ca3ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.348)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.23.5)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.69)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.1,>=0.0.12->langchain) (3.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.16.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.99)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keybert transformers langchain tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.rich import trange, tqdm\n",
        "from rich import console\n",
        "from rich.panel import Panel\n",
        "from rich.markdown import Markdown\n",
        "from rich.text import Text\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import datetime\n",
        "from rich.console import Console\n",
        "console = Console(width=110)\n",
        "from transformers import pipeline\n",
        "import os\n",
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "id": "wDMls9ExB8Oc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "kw_model = KeyBERT(model='intfloat/multilingual-e5-base')"
      ],
      "metadata": {
        "id": "h6uzqP58Ci0r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logfile = 'KeyBERT-Log.txt'\n",
        "\n",
        "def writehistory(text):\n",
        "    with open(logfile, 'a', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "        f.write('\\n')\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "CiPtv3dcCqBh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keys(text, ngram,dvsity):\n",
        "    import datetime\n",
        "    import random\n",
        "    a = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, ngram), stop_words='english',\n",
        "                              use_mmr=True, diversity=dvsity, highlight=True)     #highlight=True\n",
        "    tags = []\n",
        "    for kw in a:\n",
        "        tags.append(str(kw[0]))\n",
        "    timestamped = datetime.datetime.now()\n",
        "    #LOG THE TEXT AND THE METATAGS\n",
        "    logging_text = f\"LOGGED ON: {str(timestamped)}\\nMETADATA: {str(tags)}\\nsettings: keyphrase_ngram_range (1,{str(ngram)})  Diversity {str(dvsity)}\\n---\\nORIGINAL TEXT:\\n{text}\\n---\\n\\n\"\n",
        "    writehistory(logging_text)\n",
        "    return tags"
      ],
      "metadata": {
        "id": "Gv4yxez3CrQq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Title: Magicoder: Source Code Is All You Need\n",
        "published on PaperswithCode\n",
        "Authors: Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang\n",
        "url: https://arxiv.org/pdf/2312.02120v1.pdf\n",
        "We introduce Magicoder, a series of fully open-source (code, weights, and data)\n",
        "Large Language Models (LLMs) for code that significantly closes the gap with top\n",
        "code models while having no more than 7B parameters. Magicoder models are\n",
        "trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach\n",
        "to enlightening LLMs with open-source code snippets to generate high-quality\n",
        "instruction data for code. Our main motivation is to mitigate the inherent bias of\n",
        "the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the production of more diverse, realistic, and controllable\n",
        "data. The orthogonality of OSS-INSTRUCT and other data generation methods\n",
        "like Evol-Instruct further enables us to build an enhanced MagicoderS. Both\n",
        "Magicoder and MagicoderS substantially outperform state-of-the-art code models\n",
        "with similar or even larger sizes on a wide range of coding benchmarks, including\n",
        "Python text-to-code generation, multilingual coding, and data-science program\n",
        "completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses\n",
        "the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall,\n",
        "OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction\n",
        "tuning using abundant open-source references.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "thUbzpt-CrTB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = extract_keys(text, 1,0.32)\n",
        "console.print(f\"[bold]Keywords: {a}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "9K4MNSGACrWj",
        "outputId": "1091edd6-69dc-4cea-feca-400817dbaff7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Title \u001b[30;48;2;255;255;0mMagicoder\u001b[0m Source Code Is All You Need published on \u001b[30;48;2;255;255;0mPaperswithCode\u001b[0m Authors Yuxiang Wei Zhe Wang Jiawei Liu \n",
              "Yifeng Ding Lingming Zhang url https arxiv org pdf 2312 02120v1 pdf We introduce \u001b[30;48;2;255;255;0mMagicoder\u001b[0m series of fully open \n",
              "source code weights and data Large Language Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m for code that significantly closes the gap with top code \n",
              "models while having no more than 7B parameters \u001b[30;48;2;255;255;0mMagicoder\u001b[0m models are trained on 75K synthetic instruction data using\n",
              "OSS INSTRUCT novel approach to enlightening \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source code snippets to generate high quality \n",
              "instruction data for code Our main motivation is to mitigate the inherent bias of the synthetic data generated by \n",
              "\u001b[30;48;2;255;255;0mLLMs\u001b[0m by empowering them with wealth of \u001b[30;48;2;255;255;0mopensource\u001b[0m references for the production of more diverse realistic and \n",
              "controllable data The orthogonality of OSS INSTRUCT and other data generation methods like Evol Instruct further \n",
              "enables us to build an enhanced MagicoderS Both \u001b[30;48;2;255;255;0mMagicoder\u001b[0m and MagicoderS substantially outperform state of the art \n",
              "code models with similar or even larger sizes on wide range of coding benchmarks including Python text to code \n",
              "generation multilingual coding and data science program completion Notably MagicoderS CL 7B based on CODELLAMA even\n",
              "surpasses the prominent \u001b[30;48;2;255;255;0mChatGPT\u001b[0m on HumanEval 66 vs 65 in pass Overall OSS INSTRUCT opens new direction for low bias\n",
              "and high quality instruction tuning using abundant open source references\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Title <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Source Code Is All You Need published on <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">PaperswithCode</span> Authors Yuxiang Wei Zhe Wang Jiawei Liu \n",
              "Yifeng Ding Lingming Zhang url https arxiv org pdf 2312 02120v1 pdf We introduce <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> series of fully open \n",
              "source code weights and data Large Language Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> for code that significantly closes the gap with top code \n",
              "models while having no more than 7B parameters <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> models are trained on 75K synthetic instruction data using\n",
              "OSS INSTRUCT novel approach to enlightening <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source code snippets to generate high quality \n",
              "instruction data for code Our main motivation is to mitigate the inherent bias of the synthetic data generated by \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> by empowering them with wealth of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">opensource</span> references for the production of more diverse realistic and \n",
              "controllable data The orthogonality of OSS INSTRUCT and other data generation methods like Evol Instruct further \n",
              "enables us to build an enhanced MagicoderS Both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> and MagicoderS substantially outperform state of the art \n",
              "code models with similar or even larger sizes on wide range of coding benchmarks including Python text to code \n",
              "generation multilingual coding and data science program completion Notably MagicoderS CL 7B based on CODELLAMA even\n",
              "surpasses the prominent <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on HumanEval 66 vs 65 in pass Overall OSS INSTRUCT opens new direction for low bias\n",
              "and high quality instruction tuning using abundant open source references\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mKeywords: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;32m'magicoder'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'opensource'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'llms'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'paperswithcode'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;32m'chatgpt'\u001b[0m\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Keywords: [</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'magicoder'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'opensource'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'llms'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'paperswithcode'</span><span style=\"font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'chatgpt'</span><span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Magicoder Source Code Is All You Need.txt'\n",
        "with open(filename, encoding=\"utf8\") as f:\n",
        "  fulltext = f.read()\n",
        "f.close()\n",
        "console.print(\"Text has been saved into variable [bold]fulltext\")\n",
        "title = 'Magicoder: Source Code Is All You Need'\n",
        "filename = '2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'\n",
        "author = 'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'\n",
        "url = 'https://arxiv.org/pdf/2312.02120v1.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gsElwXiXDwOe",
        "outputId": "8fde6151-d757-47e0-bb9a-97cafca7630e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text has been saved into variable \u001b[1mfulltext\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Text has been saved into variable <span style=\"font-weight: bold\">fulltext</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import TokenTextSplitter"
      ],
      "metadata": {
        "id": "Mmi9Q4tbDGDE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = TokenTextSplitter(chunk_size=350, chunk_overlap=10)\n",
        "splitted_text = text_splitter.split_text(fulltext)"
      ],
      "metadata": {
        "id": "IK0WCRC6Dens"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(len(splitted_text))\n",
        "console.print(\"---\")\n",
        "console.print(splitted_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "nHoWBVc0Dl4p",
        "outputId": "6561c74b-84de-4d7d-8d67-866dd23ba2fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m5\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "---\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">---\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Magicoder: Source Code Is All You Need\n",
              "We introduce Magicoder, a series of fully open-source \u001b[1m(\u001b[0mcode, weights, and data\u001b[1m)\u001b[0m\n",
              "Large Language Models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m for code that significantly closes the gap with top\n",
              "code models while having no more than 7B parameters. Magicoder models are\n",
              "trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach\n",
              "to enlightening LLMs with open-source code snippets to generate high-quality\n",
              "instruction data for code. Our main motivation is to mitigate the inherent bias of\n",
              "the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the \n",
              "production of more diverse, realistic, and controllable\n",
              "data. The orthogonality of OSS-INSTRUCT and other data generation methods\n",
              "like Evol-Instruct further enables us to build an enhanced MagicoderS. Both\n",
              "Magicoder and MagicoderS substantially outperform state-of-the-art code models\n",
              "with similar or even larger sizes on a wide range of coding benchmarks, including\n",
              "Python text-to-code generation, multilingual coding, and data-science program\n",
              "completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses\n",
              "the prominent ChatGPT on HumanEval+ \u001b[1m(\u001b[0m\u001b[1;36m66.5\u001b[0m vs. \u001b[1;36m65.9\u001b[0m in pass@\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m. Overall,\n",
              "OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction\n",
              "tuning using abundant open-source references.\n",
              "\n",
              "\u001b[1;36m1\u001b[0m Introduction\n",
              "Code generation, also known as program synthesis \u001b[1m[\u001b[0mGulwani et al., \u001b[1;36m2017\u001b[0m\u001b[1m]\u001b[0m, is a long-standing challenge in \n",
              "computer science. In the past\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Magicoder: Source Code Is All You Need\n",
              "We introduce Magicoder, a series of fully open-source <span style=\"font-weight: bold\">(</span>code, weights, and data<span style=\"font-weight: bold\">)</span>\n",
              "Large Language Models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> for code that significantly closes the gap with top\n",
              "code models while having no more than 7B parameters. Magicoder models are\n",
              "trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach\n",
              "to enlightening LLMs with open-source code snippets to generate high-quality\n",
              "instruction data for code. Our main motivation is to mitigate the inherent bias of\n",
              "the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the \n",
              "production of more diverse, realistic, and controllable\n",
              "data. The orthogonality of OSS-INSTRUCT and other data generation methods\n",
              "like Evol-Instruct further enables us to build an enhanced MagicoderS. Both\n",
              "Magicoder and MagicoderS substantially outperform state-of-the-art code models\n",
              "with similar or even larger sizes on a wide range of coding benchmarks, including\n",
              "Python text-to-code generation, multilingual coding, and data-science program\n",
              "completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses\n",
              "the prominent ChatGPT on HumanEval+ <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66.5</span> vs. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65.9</span> in pass@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>. Overall,\n",
              "OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction\n",
              "tuning using abundant open-source references.\n",
              "\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> Introduction\n",
              "Code generation, also known as program synthesis <span style=\"font-weight: bold\">[</span>Gulwani et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"font-weight: bold\">]</span>, is a long-standing challenge in \n",
              "computer science. In the past\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = []\n",
        "for i in trange(0,len(splitted_text)):\n",
        "  text = splitted_text[i]\n",
        "  keys.append({'document' : filename,\n",
        "              'title' : title,\n",
        "              'author' : author,\n",
        "              'url' : url,\n",
        "              'doc': text,\n",
        "              'keywords' : extract_keys(text, 1, 0.34)\n",
        "  })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969,
          "referenced_widgets": [
            "1fd711e81d6e45b283332c74569b93c9",
            "d9062e5146d24600b2596f3475658ede"
          ]
        },
        "id": "7Z_mU9nODl8C",
        "outputId": "9c4e5436-8ef0-4e78-f750-0df9ca56b784"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fd711e81d6e45b283332c74569b93c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[30;48;2;255;255;0mMagicoder\u001b[0m Source \u001b[30;48;2;255;255;0mCode\u001b[0m Is All You Need We introduce \u001b[30;48;2;255;255;0mMagicoder\u001b[0m series of fully open source \u001b[30;48;2;255;255;0mcode\u001b[0m weights and data \n",
              "Large Language Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m for \u001b[30;48;2;255;255;0mcode\u001b[0m that significantly closes the gap with top \u001b[30;48;2;255;255;0mcode\u001b[0m models while having no more \n",
              "than 7B parameters \u001b[30;48;2;255;255;0mMagicoder\u001b[0m models are trained on 75K synthetic instruction data using OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m novel approach\n",
              "to enlightening \u001b[30;48;2;255;255;0mLLMs\u001b[0m with open source \u001b[30;48;2;255;255;0mcode\u001b[0m snippets to generate high quality instruction data for \u001b[30;48;2;255;255;0mcode\u001b[0m Our main \n",
              "motivation is to mitigate the inherent bias of the synthetic data generated by \u001b[30;48;2;255;255;0mLLMs\u001b[0m by empowering them with wealth \n",
              "of \u001b[30;48;2;255;255;0mopensource\u001b[0m references for the production of more diverse realistic and controllable data The orthogonality of \n",
              "OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m and other data generation methods like Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m further enables us to build an enhanced \n",
              "MagicoderS Both \u001b[30;48;2;255;255;0mMagicoder\u001b[0m and MagicoderS substantially outperform state of the art \u001b[30;48;2;255;255;0mcode\u001b[0m models with similar or even\n",
              "larger sizes on wide range of coding benchmarks including Python text to \u001b[30;48;2;255;255;0mcode\u001b[0m generation multilingual coding and \n",
              "data science program completion Notably MagicoderS CL 7B based on CODELLAMA even surpasses the prominent ChatGPT on\n",
              "HumanEval 66 vs 65 in pass Overall OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m opens new direction for low bias and high quality instruction \n",
              "tuning using abundant open source references Introduction \u001b[30;48;2;255;255;0mCode\u001b[0m generation also known as program synthesis Gulwani \n",
              "et al 2017 is long standing challenge in computer science In the past\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> Source <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> Is All You Need We introduce <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> series of fully open source <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> weights and data \n",
              "Large Language Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> that significantly closes the gap with top <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> models while having no more \n",
              "than 7B parameters <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> models are trained on 75K synthetic instruction data using OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> novel approach\n",
              "to enlightening <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> with open source <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> snippets to generate high quality instruction data for <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> Our main \n",
              "motivation is to mitigate the inherent bias of the synthetic data generated by <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> by empowering them with wealth \n",
              "of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">opensource</span> references for the production of more diverse realistic and controllable data The orthogonality of \n",
              "OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> and other data generation methods like Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> further enables us to build an enhanced \n",
              "MagicoderS Both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Magicoder</span> and MagicoderS substantially outperform state of the art <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> models with similar or even\n",
              "larger sizes on wide range of coding benchmarks including Python text to <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> generation multilingual coding and \n",
              "data science program completion Notably MagicoderS CL 7B based on CODELLAMA even surpasses the prominent ChatGPT on\n",
              "HumanEval 66 vs 65 in pass Overall OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> opens new direction for low bias and high quality instruction \n",
              "tuning using abundant open source references Introduction <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> generation also known as program synthesis Gulwani \n",
              "et al 2017 is long standing challenge in computer science In the past\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "standing challenge in computer science In the past few decades large body of research has been studying symbolic \n",
              "approaches such as abstraction based synthesis Wang et al 2017 Feng et al 2018 for general purpose synthesis \n",
              "problems and \u001b[30;48;2;255;255;0mprogramming\u001b[0m by examples Cambronero et al 2023 Liu et al 2023a for domain specific tasks Until recently\n",
              "Large Language Models \u001b[30;48;2;255;255;0mLLMs\u001b[0m trained on code Austin et al 2021 Chen et al 2021 has shown outstanding breakthroughs in\n",
              "generating code that accurately satisfies user intents and they are widely deployed to assist real world software \n",
              "development Microsoft 2023b Services 2023 Initially closed source models such as GPT Turbo OpenAI \u001b[30;48;2;255;255;0m2022\u001b[0m \u001b[30;48;2;255;255;0mChatGPT\u001b[0m and \n",
              "GPT4 OpenAI 2023 massively dominated various code generation benchmarks and leaderboards Chen et al 2021 Austin et \n",
              "al 2021 Liu et al 2023b Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m To further push the boundaries of code generation with open source \u001b[30;48;2;255;255;0mLLMs\u001b[0m SELF\n",
              "INSTRUCT Wang et al 2023a is adopted to bootstrap the instruction following ability of \u001b[30;48;2;255;255;0mLLMs\u001b[0m In the realm of code \n",
              "practitioners commonly devise synthetic coding instructions using stronger teacher model \u001b[30;48;2;255;255;0mChatGPT\u001b[0m and GPT and then \n",
              "finetune weaker student model \u001b[30;48;2;255;255;0mCODELLAMA\u001b[0m Rozière et al 2023 with the generated data to distill the\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">standing challenge in computer science In the past few decades large body of research has been studying symbolic \n",
              "approaches such as abstraction based synthesis Wang et al 2017 Feng et al 2018 for general purpose synthesis \n",
              "problems and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">programming</span> by examples Cambronero et al 2023 Liu et al 2023a for domain specific tasks Until recently\n",
              "Large Language Models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> trained on code Austin et al 2021 Chen et al 2021 has shown outstanding breakthroughs in\n",
              "generating code that accurately satisfies user intents and they are widely deployed to assist real world software \n",
              "development Microsoft 2023b Services 2023 Initially closed source models such as GPT Turbo OpenAI <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> and \n",
              "GPT4 OpenAI 2023 massively dominated various code generation benchmarks and leaderboards Chen et al 2021 Austin et \n",
              "al 2021 Liu et al 2023b Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> To further push the boundaries of code generation with open source <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> SELF\n",
              "INSTRUCT Wang et al 2023a is adopted to bootstrap the instruction following ability of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">LLMs</span> In the realm of code \n",
              "practitioners commonly devise synthetic coding instructions using stronger teacher model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> and GPT and then \n",
              "finetune weaker student model <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">CODELLAMA</span> Rozière et al 2023 with the generated data to distill the\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "with the generated data to distill the knowledge from the teacher Taori et al 2023 Chaudhary 2023 For example \u001b[30;48;2;255;255;0mCode\u001b[0m \n",
              "\u001b[30;48;2;255;255;0mAlpaca\u001b[0m Chaudhary 2023 consists of 20K automatically generated \u001b[30;48;2;255;255;0mcode\u001b[0m instructions by applying SELF \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m on \n",
              "\u001b[30;48;2;255;255;0mChatGPT\u001b[0m using 21 seed tasks To further enhance the coding abilities of LLMs Luo et al 2023b proposes \u001b[30;48;2;255;255;0mCode\u001b[0m \u001b[30;48;2;255;255;0mEvol\u001b[0m \n",
              "\u001b[30;48;2;255;255;0mInstruct\u001b[0m that employs various heuristics to increase the complexity of seed \u001b[30;48;2;255;255;0mcode\u001b[0m instructions \u001b[30;48;2;255;255;0mCode\u001b[0m \u001b[30;48;2;255;255;0mAlpaca\u001b[0m in this \n",
              "case achieving state of the art SOTA results among open source models While these data generation methods can \n",
              "effectively improve the instruction following capability of an LLM they rely on narrow range of predefined tasks or\n",
              "heuristics under the hood For example on the one hand \u001b[30;48;2;255;255;0mCode\u001b[0m \u001b[30;48;2;255;255;0mAlpaca\u001b[0m that adopts SELF \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m only relies on 21 seed \n",
              "tasks to generate new \u001b[30;48;2;255;255;0mcode\u001b[0m instructions using an identical prompt template On the other hand \u001b[30;48;2;255;255;0mCode\u001b[0m \u001b[30;48;2;255;255;0mEvol\u001b[0m \u001b[30;48;2;255;255;0mInstruct\u001b[0m \n",
              "takes \u001b[30;48;2;255;255;0mCode\u001b[0m \u001b[30;48;2;255;255;0mAlpaca\u001b[0m as seeds and merely depends on heuristics to evolve the dataset As partly suggested by Yu et al \n",
              "2023 and Wang et al 2023a such approaches may significantly inherit the system bias inherent in the LLMs as well as\n",
              "the predefined tasks Therefore in this paper we propose OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m to mitigate the inherent bias of LLMs and to \n",
              "unleash their potential to craft high quality and creative \u001b[30;48;2;255;255;0mcode\u001b[0m instructions via direct learning from the open \n",
              "source As\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">with the generated data to distill the knowledge from the teacher Taori et al 2023 Chaudhary 2023 For example <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> Chaudhary 2023 consists of 20K automatically generated <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> instructions by applying SELF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> on \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> using 21 seed tasks To further enhance the coding abilities of LLMs Luo et al 2023b proposes <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Evol</span> \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> that employs various heuristics to increase the complexity of seed <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> instructions <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> in this \n",
              "case achieving state of the art SOTA results among open source models While these data generation methods can \n",
              "effectively improve the instruction following capability of an LLM they rely on narrow range of predefined tasks or\n",
              "heuristics under the hood For example on the one hand <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> that adopts SELF <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> only relies on 21 seed \n",
              "tasks to generate new <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> instructions using an identical prompt template On the other hand <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Evol</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> \n",
              "takes <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Code</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Alpaca</span> as seeds and merely depends on heuristics to evolve the dataset As partly suggested by Yu et al \n",
              "2023 and Wang et al 2023a such approaches may significantly inherit the system bias inherent in the LLMs as well as\n",
              "the predefined tasks Therefore in this paper we propose OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> to mitigate the inherent bias of LLMs and to \n",
              "unleash their potential to craft high quality and creative <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> instructions via direct learning from the open \n",
              "source As\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "via direct learning from the open source As shown in Figure OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m leverages powerful LLM to automatically \n",
              "generate new \u001b[30;48;2;255;255;0mcoding\u001b[0m problems by drawing inspiration from any random code snippets collected from the open source In\n",
              "this example the LLM gets inspired by two incomplete code fragments from different functions and manages to relate \n",
              "them and craft realistic machine learning problem Thanks to the infinite real world open source code OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m \n",
              "can directly produce diverse realistic and controllable code instructions by providing distinct seed code snippets \n",
              "In the end we generate 75K synthetic data to finetune CODELLAMA PYTHON 7B resulting in Magicoder CL While being \n",
              "simple and effective OSS \u001b[30;48;2;255;255;0mINSTRUCT\u001b[0m is orthogonal to existing data generation methods and they can be combined to \n",
              "further push the boundaries of the models \u001b[30;48;2;255;255;0mcoding\u001b[0m capabilities Therefore we continually finetune Magicoder CL on an \n",
              "open source Evol \u001b[30;48;2;255;255;0mInstruct\u001b[0m with 110K entries producing \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m CL We evaluate Magicoder and \u001b[30;48;2;255;255;0mMagicoderS\u001b[0m on wide \n",
              "range of \u001b[30;48;2;255;255;0mcoding\u001b[0m tasks including HumanEval Chen et al 2021 and MBPP Austin et al 2021 for Python text to code \n",
              "generation \u001b[30;48;2;255;255;0mMultiPL\u001b[0m Cassano et al \u001b[30;48;2;255;255;0m2022\u001b[0m for multilingual code completion and DS 1000 Lai et al \u001b[30;48;2;255;255;0m2022\u001b[0m for solving data \n",
              "science problems We further adopt EvalPlus Liu et al 2023b which includes the augmented HumanEval and MBPP\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">via direct learning from the open source As shown in Figure OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> leverages powerful LLM to automatically \n",
              "generate new <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> problems by drawing inspiration from any random code snippets collected from the open source In\n",
              "this example the LLM gets inspired by two incomplete code fragments from different functions and manages to relate \n",
              "them and craft realistic machine learning problem Thanks to the infinite real world open source code OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> \n",
              "can directly produce diverse realistic and controllable code instructions by providing distinct seed code snippets \n",
              "In the end we generate 75K synthetic data to finetune CODELLAMA PYTHON 7B resulting in Magicoder CL While being \n",
              "simple and effective OSS <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">INSTRUCT</span> is orthogonal to existing data generation methods and they can be combined to \n",
              "further push the boundaries of the models <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> capabilities Therefore we continually finetune Magicoder CL on an \n",
              "open source Evol <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">Instruct</span> with 110K entries producing <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> CL We evaluate Magicoder and <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderS</span> on wide \n",
              "range of <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">coding</span> tasks including HumanEval Chen et al 2021 and MBPP Austin et al 2021 for Python text to code \n",
              "generation <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MultiPL</span> Cassano et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> for multilingual code completion and DS 1000 Lai et al <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">2022</span> for solving data \n",
              "science problems We further adopt EvalPlus Liu et al 2023b which includes the augmented HumanEval and MBPP\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gmented HumanEval and MBPP datasets for more rigorous model evaluation Both \u001b[30;48;2;255;255;0mMagicoderCL\u001b[0m and MagicoderS CL \n",
              "substantially boost the base CODELLAMA PYTHON 7B Additionally Magicoder CL even outperforms \u001b[30;48;2;255;255;0mWizardCoder\u001b[0m CL 7B \n",
              "\u001b[30;48;2;255;255;0mWizardCoder\u001b[0m SC 15B and all studied SOTA LLMs with less than or equal to 16B parameters on all the benchmarks we \n",
              "tested Also the pass result of the \u001b[30;48;2;255;255;0menhanced\u001b[0m MagicoderS CL is on par with \u001b[30;48;2;255;255;0mChatGPT\u001b[0m on HumanEval 70 vs 72 and \n",
              "surpasses it on the more rigorous HumanEval 66 vs 65 indicating that MagicoderS CL can generate more robust \u001b[30;48;2;255;255;0mcode\u001b[0m It\n",
              "also achieves SOTA results among all \u001b[30;48;2;255;255;0mcode\u001b[0m models at the same scale\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">gmented HumanEval and MBPP datasets for more rigorous model evaluation Both <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">MagicoderCL</span> and MagicoderS CL \n",
              "substantially boost the base CODELLAMA PYTHON 7B Additionally Magicoder CL even outperforms <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> CL 7B \n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">WizardCoder</span> SC 15B and all studied SOTA LLMs with less than or equal to 16B parameters on all the benchmarks we \n",
              "tested Also the pass result of the <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">enhanced</span> MagicoderS CL is on par with <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">ChatGPT</span> on HumanEval 70 vs 72 and \n",
              "surpasses it on the more rigorous HumanEval 66 vs 65 indicating that MagicoderS CL can generate more robust <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> It\n",
              "also achieves SOTA results among all <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">code</span> models at the same scale\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(keys[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "8_FZJXm9DerC",
        "outputId": "ceb6b2dd-10c5-4b86-aa80-91fe16329e2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'document'\u001b[0m: \u001b[32m'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'\u001b[0m,\n",
              "    \u001b[32m'title'\u001b[0m: \u001b[32m'Magicoder: Source Code Is All You Need'\u001b[0m,\n",
              "    \u001b[32m'author'\u001b[0m: \u001b[32m'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'\u001b[0m,\n",
              "    \u001b[32m'url'\u001b[0m: \u001b[32m'https://arxiv.org/pdf/2312.02120v1.pdf'\u001b[0m,\n",
              "    \u001b[32m'doc'\u001b[0m: \u001b[32m'-standing challenge in computer science. In the past few decades, a large body of research has \u001b[0m\n",
              "\u001b[32mbeen studying\\nsymbolic approaches, such as abstraction-based synthesis \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWang et al., 2017, Feng et al., 2018\u001b[0m\u001b[32m]\u001b[0m\n",
              "\u001b[32mfor\\ngeneral-purpose synthesis problems and programming by examples \u001b[0m\u001b[32m[\u001b[0m\u001b[32mCambronero et al., 2023, Liu\\net al., \u001b[0m\n",
              "\u001b[32m2023a\u001b[0m\u001b[32m]\u001b[0m\u001b[32m for domain-specific tasks. Until recently, Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m trained on\\ncode \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAustin et \u001b[0m\n",
              "\u001b[32mal., 2021, Chen et al., 2021\u001b[0m\u001b[32m]\u001b[0m\u001b[32m has shown outstanding breakthroughs in generating\\ncode that accurately \u001b[0m\n",
              "\u001b[32msatisfies user intents, and they are widely deployed to assist real-world software\\ndevelopment \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMicrosoft, \u001b[0m\n",
              "\u001b[32m2023b, Services, 2023\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nInitially, closed-source models such as GPT-3.5 Turbo \u001b[0m\u001b[32m[\u001b[0m\u001b[32mOpenAI, 2022\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., ChatGPT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32mand GPT4 \u001b[0m\u001b[32m[\u001b[0m\u001b[32mOpenAI, 2023\u001b[0m\u001b[32m]\u001b[0m\u001b[32m massively dominated various code generation benchmarks and leaderboards \u001b[0m\u001b[32m[\u001b[0m\u001b[32mChen\\net al.,\u001b[0m\n",
              "\u001b[32m2021, Austin et al., 2021, Liu et al., 2023b, Lai et al., 2022\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. To further push the boundaries\\nof code \u001b[0m\n",
              "\u001b[32mgeneration with open source LLMs, SELF-INSTRUCT \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWang et al., 2023a\u001b[0m\u001b[32m]\u001b[0m\u001b[32m is adopted to\\nbootstrap the \u001b[0m\n",
              "\u001b[32minstruction-following ability of LLMs. In the realm of code, practitioners commonly\\ndevise synthetic coding \u001b[0m\n",
              "\u001b[32minstructions using a stronger teacher model \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., ChatGPT and GPT-4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and\\nthen finetune a weaker student \u001b[0m\n",
              "\u001b[32mmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., CODELLAMA \u001b[0m\u001b[32m[\u001b[0m\u001b[32mRozière et al., 2023\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with the generated\\ndata to distill the'\u001b[0m,\n",
              "    \u001b[32m'keywords'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'programming'\u001b[0m, \u001b[32m'2022'\u001b[0m, \u001b[32m'llms'\u001b[0m, \u001b[32m'chatgpt'\u001b[0m, \u001b[32m'codellama'\u001b[0m\u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'document'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Magicoder: Source Code Is All You Need'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://arxiv.org/pdf/2312.02120v1.pdf'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'doc'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'-standing challenge in computer science. In the past few decades, a large body of research has </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">been studying\\nsymbolic approaches, such as abstraction-based synthesis [Wang et al., 2017, Feng et al., 2018]</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">for\\ngeneral-purpose synthesis problems and programming by examples [Cambronero et al., 2023, Liu\\net al., </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2023a] for domain-specific tasks. Until recently, Large Language Models (LLMs) trained on\\ncode [Austin et </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2021, Chen et al., 2021] has shown outstanding breakthroughs in generating\\ncode that accurately </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">satisfies user intents, and they are widely deployed to assist real-world software\\ndevelopment [Microsoft, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2023b, Services, 2023].\\nInitially, closed-source models such as GPT-3.5 Turbo [OpenAI, 2022] (i.e., ChatGPT) </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and GPT4 [OpenAI, 2023] massively dominated various code generation benchmarks and leaderboards [Chen\\net al.,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2021, Austin et al., 2021, Liu et al., 2023b, Lai et al., 2022]. To further push the boundaries\\nof code </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generation with open source LLMs, SELF-INSTRUCT [Wang et al., 2023a] is adopted to\\nbootstrap the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">instruction-following ability of LLMs. In the realm of code, practitioners commonly\\ndevise synthetic coding </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">instructions using a stronger teacher model (e.g., ChatGPT and GPT-4) and\\nthen finetune a weaker student </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model (e.g., CODELLAMA [Rozière et al., 2023]) with the generated\\ndata to distill the'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'programming'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'2022'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'llms'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chatgpt'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'codellama'</span><span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############### CREATE cHUnKS DOC DATABASE ##################\n",
        "from langchain.schema.document import Document\n",
        "goodDocs = []\n",
        "for i in range(0,len(keys)):\n",
        "  goodDocs.append(Document(page_content = keys[i]['doc'],\n",
        "                          metadata = {'source': keys[i]['document'],\n",
        "                              'type': 'chunk',\n",
        "                              'title': keys[i]['title'],\n",
        "                              'author': keys[i]['author'],\n",
        "                              'url' : keys[i]['url'],\n",
        "                              'keywords' : keys[i]['keywords']\n",
        "                              }))"
      ],
      "metadata": {
        "id": "0Ru5xqHHuTFL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console.print(goodDocs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "LfizXLJruW6m",
        "outputId": "4d86895f-753d-4dc9-cd12-afb81cdb3fb1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mpage_content\u001b[0m=\u001b[32m'-standing challenge in computer science. In the past few decades, a large body of research \u001b[0m\n",
              "\u001b[32mhas been studying\\nsymbolic approaches, such as abstraction-based synthesis \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWang et al., 2017, Feng et al., \u001b[0m\n",
              "\u001b[32m2018\u001b[0m\u001b[32m]\u001b[0m\u001b[32m for\\ngeneral-purpose synthesis problems and programming by examples \u001b[0m\u001b[32m[\u001b[0m\u001b[32mCambronero et al., 2023, Liu\\net \u001b[0m\n",
              "\u001b[32mal., 2023a\u001b[0m\u001b[32m]\u001b[0m\u001b[32m for domain-specific tasks. Until recently, Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m trained on\\ncode \u001b[0m\u001b[32m[\u001b[0m\u001b[32mAustin \u001b[0m\n",
              "\u001b[32met al., 2021, Chen et al., 2021\u001b[0m\u001b[32m]\u001b[0m\u001b[32m has shown outstanding breakthroughs in generating\\ncode that accurately \u001b[0m\n",
              "\u001b[32msatisfies user intents, and they are widely deployed to assist real-world software\\ndevelopment \u001b[0m\u001b[32m[\u001b[0m\u001b[32mMicrosoft, \u001b[0m\n",
              "\u001b[32m2023b, Services, 2023\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nInitially, closed-source models such as GPT-3.5 Turbo \u001b[0m\u001b[32m[\u001b[0m\u001b[32mOpenAI, 2022\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., ChatGPT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32mand GPT4 \u001b[0m\u001b[32m[\u001b[0m\u001b[32mOpenAI, 2023\u001b[0m\u001b[32m]\u001b[0m\u001b[32m massively dominated various code generation benchmarks and leaderboards \u001b[0m\u001b[32m[\u001b[0m\u001b[32mChen\\net al.,\u001b[0m\n",
              "\u001b[32m2021, Austin et al., 2021, Liu et al., 2023b, Lai et al., 2022\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. To further push the boundaries\\nof code \u001b[0m\n",
              "\u001b[32mgeneration with open source LLMs, SELF-INSTRUCT \u001b[0m\u001b[32m[\u001b[0m\u001b[32mWang et al., 2023a\u001b[0m\u001b[32m]\u001b[0m\u001b[32m is adopted to\\nbootstrap the \u001b[0m\n",
              "\u001b[32minstruction-following ability of LLMs. In the realm of code, practitioners commonly\\ndevise synthetic coding \u001b[0m\n",
              "\u001b[32minstructions using a stronger teacher model \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., ChatGPT and GPT-4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and\\nthen finetune a weaker student \u001b[0m\n",
              "\u001b[32mmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., CODELLAMA \u001b[0m\u001b[32m[\u001b[0m\u001b[32mRozière et al., 2023\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with the generated\\ndata to distill the'\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
              "        \u001b[32m'source'\u001b[0m: \u001b[32m'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'\u001b[0m,\n",
              "        \u001b[32m'type'\u001b[0m: \u001b[32m'chunk'\u001b[0m,\n",
              "        \u001b[32m'title'\u001b[0m: \u001b[32m'Magicoder: Source Code Is All You Need'\u001b[0m,\n",
              "        \u001b[32m'author'\u001b[0m: \u001b[32m'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'\u001b[0m,\n",
              "        \u001b[32m'url'\u001b[0m: \u001b[32m'https://arxiv.org/pdf/2312.02120v1.pdf'\u001b[0m,\n",
              "        \u001b[32m'keywords'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'programming'\u001b[0m, \u001b[32m'2022'\u001b[0m, \u001b[32m'llms'\u001b[0m, \u001b[32m'chatgpt'\u001b[0m, \u001b[32m'codellama'\u001b[0m\u001b[1m]\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'-standing challenge in computer science. In the past few decades, a large body of research </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">has been studying\\nsymbolic approaches, such as abstraction-based synthesis [Wang et al., 2017, Feng et al., </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2018] for\\ngeneral-purpose synthesis problems and programming by examples [Cambronero et al., 2023, Liu\\net </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2023a] for domain-specific tasks. Until recently, Large Language Models (LLMs) trained on\\ncode [Austin </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2021, Chen et al., 2021] has shown outstanding breakthroughs in generating\\ncode that accurately </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">satisfies user intents, and they are widely deployed to assist real-world software\\ndevelopment [Microsoft, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2023b, Services, 2023].\\nInitially, closed-source models such as GPT-3.5 Turbo [OpenAI, 2022] (i.e., ChatGPT) </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and GPT4 [OpenAI, 2023] massively dominated various code generation benchmarks and leaderboards [Chen\\net al.,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2021, Austin et al., 2021, Liu et al., 2023b, Lai et al., 2022]. To further push the boundaries\\nof code </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generation with open source LLMs, SELF-INSTRUCT [Wang et al., 2023a] is adopted to\\nbootstrap the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">instruction-following ability of LLMs. In the realm of code, practitioners commonly\\ndevise synthetic coding </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">instructions using a stronger teacher model (e.g., ChatGPT and GPT-4) and\\nthen finetune a weaker student </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model (e.g., CODELLAMA [Rozière et al., 2023]) with the generated\\ndata to distill the'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chunk'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Magicoder: Source Code Is All You Need'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'https://arxiv.org/pdf/2312.02120v1.pdf'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'programming'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'2022'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'llms'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chatgpt'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'codellama'</span><span style=\"font-weight: bold\">]</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5rwq5MnuZyk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}