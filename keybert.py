# -*- coding: utf-8 -*-
"""KeyBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SFe8UGgd7SWE-dtcgajMpaoFm0UBudjf
"""

!pip install keybert transformers langchain tiktoken

from tqdm.rich import trange, tqdm
from rich import console
from rich.panel import Panel
from rich.markdown import Markdown
from rich.text import Text
import warnings
warnings.filterwarnings(action='ignore')
import datetime
from rich.console import Console
console = Console(width=110)
from transformers import pipeline
import os
from keybert import KeyBERT

from keybert import KeyBERT
kw_model = KeyBERT(model='intfloat/multilingual-e5-base')

logfile = 'KeyBERT-Log.txt'

def writehistory(text):
    with open(logfile, 'a', encoding='utf-8') as f:
        f.write(text)
        f.write('\n')
    f.close()

def extract_keys(text, ngram,dvsity):
    import datetime
    import random
    a = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, ngram), stop_words='english',
                              use_mmr=True, diversity=dvsity, highlight=True)     #highlight=True
    tags = []
    for kw in a:
        tags.append(str(kw[0]))
    timestamped = datetime.datetime.now()
    #LOG THE TEXT AND THE METATAGS
    logging_text = f"LOGGED ON: {str(timestamped)}\nMETADATA: {str(tags)}\nsettings: keyphrase_ngram_range (1,{str(ngram)})  Diversity {str(dvsity)}\n---\nORIGINAL TEXT:\n{text}\n---\n\n"
    writehistory(logging_text)
    return tags

text = """
Title: Magicoder: Source Code Is All You Need
published on PaperswithCode
Authors: Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang
url: https://arxiv.org/pdf/2312.02120v1.pdf
We introduce Magicoder, a series of fully open-source (code, weights, and data)
Large Language Models (LLMs) for code that significantly closes the gap with top
code models while having no more than 7B parameters. Magicoder models are
trained on 75K synthetic instruction data using OSS-INSTRUCT, a novel approach
to enlightening LLMs with open-source code snippets to generate high-quality
instruction data for code. Our main motivation is to mitigate the inherent bias of
the synthetic data generated by LLMs by empowering them with a wealth of opensource references for the production of more diverse, realistic, and controllable
data. The orthogonality of OSS-INSTRUCT and other data generation methods
like Evol-Instruct further enables us to build an enhanced MagicoderS. Both
Magicoder and MagicoderS substantially outperform state-of-the-art code models
with similar or even larger sizes on a wide range of coding benchmarks, including
Python text-to-code generation, multilingual coding, and data-science program
completion. Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses
the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall,
OSS-INSTRUCT opens a new direction for low-bias and high-quality instruction
tuning using abundant open-source references.
"""

a = extract_keys(text, 1,0.32)
console.print(f"[bold]Keywords: {a}")

filename = 'Magicoder Source Code Is All You Need.txt'
with open(filename, encoding="utf8") as f:
  fulltext = f.read()
f.close()
console.print("Text has been saved into variable [bold]fulltext")
title = 'Magicoder: Source Code Is All You Need'
filename = '2023-12-03 18.41.12 Governing societies with Artificial Intelligence.txt'
author = 'Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, Lingming Zhang'
url = 'https://arxiv.org/pdf/2312.02120v1.pdf'

from langchain.document_loaders import TextLoader
from langchain.text_splitter import TokenTextSplitter

text_splitter = TokenTextSplitter(chunk_size=350, chunk_overlap=10)
splitted_text = text_splitter.split_text(fulltext)

console.print(len(splitted_text))
console.print("---")
console.print(splitted_text[0])

keys = []
for i in trange(0,len(splitted_text)):
  text = splitted_text[i]
  keys.append({'document' : filename,
              'title' : title,
              'author' : author,
              'url' : url,
              'doc': text,
              'keywords' : extract_keys(text, 1, 0.34)
  })

console.print(keys[1])

############### CREATE cHUnKS DOC DATABASE ##################
from langchain.schema.document import Document
goodDocs = []
for i in range(0,len(keys)):
  goodDocs.append(Document(page_content = keys[i]['doc'],
                          metadata = {'source': keys[i]['document'],
                              'type': 'chunk',
                              'title': keys[i]['title'],
                              'author': keys[i]['author'],
                              'url' : keys[i]['url'],
                              'keywords' : keys[i]['keywords']
                              }))

console.print(goodDocs[1])

